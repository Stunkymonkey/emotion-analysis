{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Viterbi version of target sequence labelling\n",
    "# Intro\n",
    "The viterbi algorithm for labelling sequences uses the Hidden-Markov-Model and computes the\n",
    "most probable label sequence as the combination of transition and emission probabilities for\n",
    "the given word sequence. It is most often used in POS-tagging but nicely fits our task of\n",
    "labelling emotion target sequences. As it uses not only the observed tokens but also the last\n",
    "(number of) tag, it takes the probability of tags following each other into account. As our tags\n",
    "are rigid in their sequence (Inside only follows Beginning, Beginning only follows Outside), using\n",
    "transition probabilities will improve the output labels by hopefully avoiding illegal tag transitions.\n",
    "# Method\n",
    "The used viterbi algorithm is implemented from scratch and works on the non-preprocessed tokens.\n",
    "To use it, the data has to be prepared first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import pandas"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data preparation\n",
    "As seen later, the algorithm is trained by calculating emission and transition probabilities,\n",
    "which are taken from the tokenized training data. First, the file is loaded and all unnecessary\n",
    "parts are dropped."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "with open('emotion-rl-corpora.jsonl', encoding='utf-8') as f:\n",
    "    data_source = pandas.read_json(f, lines=True)\n",
    "data_source = data_source.loc[\n",
    "    (data_source['dataset']=='reman')|(data_source['dataset']=='electoral_tweets')|(data_source['dataset']=='gne')\n",
    "]\n",
    "noanno = data_source[data_source['annotations'] == {}].index\n",
    "data_source = data_source.drop(noanno)\n",
    "data_source = data_source.drop(columns=[\n",
    "    'meta', 'steps', 'tags', 'split', 'annotation-offsets', 'extra'\n",
    "])\n",
    "\n",
    "data_source = data_source.join(data_source['annotations'].apply(pandas.Series))\n",
    "noanno = [i for i,row in data_source.iterrows() if type(row['target']) == float]\n",
    "data_source = data_source.drop(noanno)\n",
    "data_source = data_source.drop(columns=[\n",
    "    'annotations', 'cause', 'cue-joy', 'cue-sadness', 'cue-anger', 'cue-other', 'cue-disgust',\n",
    "    'cue-trust', 'cue-anticipation', 'cue-surprise', 'cue-fear', 'cue', 'experiencer'\n",
    "])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "After removing every annotation but target and all additional information other than the corpus and text id,\n",
    "the raw text itself and token and label lists, the algorithm can be trained.\n",
    "## Training\n",
    "### HMM\n",
    "The viterbi algorithm is based on a Hidden-Markov model, as the model computes the probability of a token-tag sequence\n",
    "and viterbi is a way to return the tag sequence with the highest probability. Why viterbi is necessary will be explained\n",
    "after implementing a HMM."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "class HiddenMarkovModel:\n",
    "    transitions = dict()\n",
    "    emissions = dict()\n",
    "    prior = dict()\n",
    "    observations = {'transitions': transitions, 'emissions': emissions}\n",
    "\n",
    "    def __init__(self):\n",
    "        self.transitions = {'O': {'O': 0, 'B': 0, 'I': 0}, 'B': {'O': 0, 'B': 0, 'I': 0}, 'I': {'O': 0, 'B': 0, 'I': 0}}\n",
    "        self.prior = {'O': 0, 'B': 0, 'I': 0}\n",
    "\n",
    "    def train_probabilities(self, train_set: pandas.DataFrame):\n",
    "        trans = {'O': {'O': 0, 'B': 0, 'I': 0}, 'B': {'O': 0, 'B': 0, 'I': 0}, 'I': {'O': 0, 'B': 0, 'I': 0}}\n",
    "        emi = dict()\n",
    "        prior = {'O': 0, 'B': 0, 'I': 0}\n",
    "        for index, row in train_set.iterrows():\n",
    "            # The current sequence consists of (word, tag) pairs\n",
    "            seq = list(zip(row['tokens'], row['target']))\n",
    "\n",
    "            for i in range(len(seq)):\n",
    "                tok = seq[i][0]\n",
    "                tag = seq[i][1]\n",
    "                # First tag: Prior count\n",
    "                if i == 0:\n",
    "                    prior[tag] += 1\n",
    "                # Every other tag: Transition count from the previous tag\n",
    "                else:\n",
    "                    trans[tag][seq[i - 1][1]] += 1\n",
    "                # Emission count from token-tag-pairs\n",
    "                if tok in emi:\n",
    "                    emi[tok][tag] += 1\n",
    "                else:\n",
    "                    emi[tok] = {'O': 0, 'B': 0, 'I': 0}\n",
    "                    emi[tok][tag] += 1\n",
    "        # Converting the absolute emission/transition/prior frequencies into probabilities\n",
    "        for tag in {'O', 'B', 'I'}:\n",
    "            freq = sum(trans[tag].values())\n",
    "            for prev_tag in trans[tag]:\n",
    "                trans[tag][prev_tag] /= freq\n",
    "            prior[tag] /= sum(prior.values())\n",
    "\n",
    "        for tok in emi:\n",
    "            freq = sum(emi[tok].values())\n",
    "            for tag in {'O', 'B', 'I'}:\n",
    "                emi[tok][tag] /= freq\n",
    "\n",
    "        emi['$OOV'] = {tag: sum(trans[tag].values())/sum([sum(trans[tag2].values())\n",
    "                                                              for tag2 in {'O', 'B', 'I'}]) for tag in {'O', 'B', 'I'}}\n",
    "        self.transitions = trans\n",
    "        self.emissions = emi\n",
    "        self.prior = prior"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Viterbi\n",
    "The trained HMM includes the emission probabilities as token-tag-pair-frequencies relative to token-frequency and\n",
    "transition probabilities as tag bigram frequencies relative to the frequency of the second tag. As every token in the\n",
    "sequence has three tag possibilities which are also influenced by the previous tag, there are 3<sup>n</sup> tag sequences\n",
    "for a sequence with n tokens, all with differing probabilities. Viterbi keeps track of the maximum probability of a current\n",
    "token and the maximum probability of different histories to compute the best sequence from all HMM-sequences."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def viterbi(sequence: list[str], model: HiddenMarkovModel):\n",
    "    viterbi_table = [[0 for _ in range(3)] for _ in range(len(sequence))]\n",
    "    max_labels = list()\n",
    "    # Go through the sequence token by token\n",
    "    for i_tok, token in enumerate(sequence):\n",
    "        # For each possible Tag, multiply\n",
    "        for i_tag, tag in enumerate(['O', 'B', 'I']):\n",
    "            # the emission probability of the tag and current token\n",
    "            if token in model.emissions:\n",
    "                viterbi_table[i_tok][i_tag] = model.emissions[token][tag]\n",
    "            else:\n",
    "                viterbi_table[i_tok][i_tag] = model.emissions['$OOV'][tag]\n",
    "            # with the prior probability of the tag for the first token\n",
    "            if i_tok == 0:\n",
    "                viterbi_table[i_tok][i_tag] *= model.prior[tag]\n",
    "            # or with the transition probability for best tag of the last token for all other tokens\n",
    "            else:\n",
    "                viterbi_table[i_tok][i_tag] *= max_v * model.transitions[tag][max_t]\n",
    "        max_v = max(viterbi_table[i_tok])\n",
    "        max_t = ['O', 'B', 'I'][viterbi_table[i_tok].index(max_v)]\n",
    "    indices = {0: 'O', 1: 'B', 2: 'I'}\n",
    "    # Read the best tags from the table and return the sequence\n",
    "    for i in range(len(sequence)):\n",
    "        max_labels.append(indices[viterbi_table[i].index(max(viterbi_table[i]))])\n",
    "    return max_labels"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The sequence tagger can now be trained on whichever corpus we like. To then find the most probably tags for a sequence, we can just use the viterbi algorithm,\n",
    "which takes a list of tokens and our trained model as parameters and returns a sequence of\n",
    "token-tag pairs:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def get_results(training: str):\n",
    "    test = ['reman', 'gne', 'electoral_tweets']\n",
    "    test.remove(training)\n",
    "    hmm = HiddenMarkovModel()\n",
    "    hmm.train_probabilities(train_set=data_source.loc[data_source['dataset']==f'{training}'])\n",
    "    test_data = data_source.loc[(data_source['dataset'] == f'{test[0]}')|(data_source['dataset'] == f'{test[1]}')]\n",
    "\n",
    "    out = [['predicted', 'gold']]\n",
    "    ids = []\n",
    "    for i, row in test_data.iterrows():\n",
    "        sequence = list(row['tokens'])\n",
    "        pred = viterbi(sequence, hmm)\n",
    "        ids.append(row['id'])\n",
    "        out.append([pred, list(row['target'])])\n",
    "\n",
    "    out_df = pandas.DataFrame(out[1:], columns=out[0], index=ids)\n",
    "    out_df.to_json(f'{training}_trained.json', orient='index')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Trained on GoodNewsEveryone"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "get_results('gne')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Trained on Reman"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "get_results('reman')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Trained on Electoral tweets"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "get_results('electoral_tweets')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}